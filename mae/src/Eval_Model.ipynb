{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.models_mae import mae_vit_huge_patch14_dec512d8b, MaskedAutoencoderViT\n",
    "\n",
    "model_states = torch.load(\"../model_data/mae_model.bin\")\n",
    "\n",
    "#print(f\"model_states: {model_states}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "new_ordered_dict = []\n",
    "\n",
    "for orig_key, val in model_states.items():\n",
    "    key = orig_key.replace(\"_forward_module.model.\", \"\")\n",
    "    print(orig_key, key)\n",
    "    new_ordered_dict.append((key, val))\n",
    "\n",
    "new_ordered_dict = collections.OrderedDict(new_ordered_dict)\n",
    "#new_ordered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda True\n",
      "--------------------------------------------------------------\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_buffers', '_call_impl', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_init_weights', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_version', 'add_module', 'apply', 'bfloat16', 'blocks', 'buffers', 'children', 'cls_token', 'cpu', 'cuda', 'decoder_blocks', 'decoder_embed', 'decoder_norm', 'decoder_pos_embed', 'decoder_pred', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'forward_decoder', 'forward_encoder', 'forward_loss', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'initialize_weights', 'ipu', 'load_state_dict', 'mask_token', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'norm', 'norm_pix_loss', 'parameters', 'patch_embed', 'patchify', 'pos_embed', 'random_masking', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'unpatchify', 'xpu', 'zero_grad']\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Device:\", device, torch.cuda.is_available())\n",
    "model = mae_vit_huge_patch14_dec512d8b()\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(dir(model))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "model.load_state_dict(new_ordered_dict, strict=False)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from transforms.mae_transforms import SquarePad\n",
    "from PIL import Image\n",
    "\n",
    "img_path = '../image_data/n01443537_12079.JPEG'\n",
    "img_size = 224\n",
    "#print(img_path)\n",
    "\n",
    "tfs_train = transforms.Compose([\n",
    "            SquarePad(),\n",
    "            #transforms.Resize((args.img_size, args.img_size)),\n",
    "            transforms.RandomResizedCrop(size=img_size, \n",
    "                                         scale=(0.7, 1.0), \n",
    "                                         ratio=(0.75, 1.3333333333333333)\n",
    "                                         ),\n",
    "            transforms.RandAugment(),\n",
    "            transforms.RandomVerticalFlip(0.1),\n",
    "            transforms.RandomHorizontalFlip(0.1),\n",
    "            transforms.GaussianBlur(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "image = tfs_train(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n",
      "1: torch.Size([3, 224, 224])\n",
      "2: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(model.patch_embed.img_size)\n",
    "\n",
    "print(\"1:\", image.shape)\n",
    "\n",
    "image = image.reshape(1, 3, 224, 224)\n",
    "\n",
    "print(\"2:\", image.shape)\n",
    "\n",
    "image = image.to(device)\n",
    "\n",
    "predictions = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.0324625 , -0.96839565, -1.0046719 , ..., -0.935914  ,\n",
       "         -0.91667444, -0.9400685 ],\n",
       "        [-1.0324469 , -0.96514314, -1.0056496 , ..., -0.94445115,\n",
       "         -0.9094999 , -0.93577194],\n",
       "        [-1.0236635 , -0.9530096 , -1.004213  , ..., -0.9399853 ,\n",
       "         -0.8992717 , -0.94054145],\n",
       "        ...,\n",
       "        [-0.78094304, -0.73067164, -0.86981726, ..., -0.9161948 ,\n",
       "         -0.9096669 , -0.99443096],\n",
       "        [-0.87522614, -0.8051869 , -0.9219824 , ..., -0.91071576,\n",
       "         -0.8990971 , -0.96212274],\n",
       "        [-0.88441527, -0.81232077, -0.9280543 , ..., -0.8948136 ,\n",
       "         -0.8880304 , -0.9575138 ]]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_image = predictions[1].cpu().detach().numpy()\n",
    "predicted_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 588)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_image[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "img = Image.fromarray(predicted_image[0], 'RGB')\n",
    "img.save(img_path+\".PNG\")\n",
    "print(\"Saved\")\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ.get('CUDA_PATH'))\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
