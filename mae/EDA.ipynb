{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient, Input, command, PyTorchDistribution\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml import MpiDistribution\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# SUBSCRIPTION_ID = \"2630c4b9-c627-494d-8a87-a5002e5e7f8e\"\n",
    "# RESOURCE_GROUP = \"openai-ngt\"\n",
    "# WORKSPACE_NAME = \"coraca\"\n",
    "# TRAIN_MLTABLE = \"mltable-silver-validation\"\n",
    "# VAL_MLTABLE = \"mltable-silver-validation\"\n",
    "# TRAIN_MLTABLE_VERSION = \"2023.05.19.130010\" \n",
    "# VAL_MLTABLE_VERSION = \"2023.05.19.130010\"\n",
    "\n",
    "SUBSCRIPTION_ID = \"2630c4b9-c627-494d-8a87-a5002e5e7f8e\"\n",
    "RESOURCE_GROUP = \"GODZILLA\"\n",
    "WORKSPACE_NAME = \"GODZILLA\"\n",
    "TRAIN_MLTABLE = \"godzilla_mae_train\"\n",
    "VAL_MLTABLE = \"godzilla_mae_val\"\n",
    "TRAIN_MLTABLE_VERSION = \"1\" \n",
    "VAL_MLTABLE_VERSION = \"1\"\n",
    "\n",
    "credential = DefaultAzureCredential(\n",
    "    exclude_shared_token_cache_credential=True,\n",
    "    exclude_visual_studio_code_credential=False,\n",
    ")\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP,\n",
    "    workspace_name=WORKSPACE_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_asset = ml_client.data.get(name=TRAIN_MLTABLE, version=TRAIN_MLTABLE_VERSION) #ml_client.data.get(name=\"toy_mltable_train\",version=\"1\")\n",
    "val_data_asset = ml_client.data.get(name=VAL_MLTABLE, version=VAL_MLTABLE_VERSION) #ml_client.data.get(name=\"toy_mltable_val\",version=\"1\")\n",
    "TRAIN_ASSET_ID = train_data_asset.id\n",
    "VAL_ASSET_ID = val_data_asset.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/subscriptions/2630c4b9-c627-494d-8a87-a5002e5e7f8e/resourceGroups/GODZILLA/providers/Microsoft.MachineLearningServices/workspaces/GODZILLA/data/godzilla_mae_train/versions/1',\n",
       " '/subscriptions/2630c4b9-c627-494d-8a87-a5002e5e7f8e/resourceGroups/GODZILLA/providers/Microsoft.MachineLearningServices/workspaces/GODZILLA/data/godzilla_mae_val/versions/1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_ASSET_ID, VAL_ASSET_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, get_worker_info\n",
    "import mltable\n",
    "import numpy as np\n",
    "\n",
    "class ImageDatasetMP(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_asset_id, \n",
    "        transform=None\n",
    "\n",
    "        ):\n",
    "        super(Dataset).__init__()\n",
    "\n",
    "        self.data_asset_id = data_asset_id\n",
    "        self.transform = transform\n",
    "        self.first_time=True\n",
    "\n",
    "    def _lazy_init(self):            \n",
    "        #print(f\"Trying to load mltable in lazy_init for worker {get_worker_info().id}...\")\n",
    "        training_data = mltable.load(f\"azureml:/{self.data_asset_id}\")\n",
    "        self.df = training_data.to_pandas_dataframe() \n",
    "\n",
    "        print(f\"Finished lazy_init.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        from multiprocessing import Process, Queue\n",
    "\n",
    "        q = Queue()\n",
    "\n",
    "        def __len(q):                \n",
    "            data = mltable.load(f\"azureml:/{self.data_asset_id}\")\n",
    "            data = data.to_pandas_dataframe()\n",
    "            q.put((data['filename_fixed'].nunique()))\n",
    "\n",
    "        p = Process(target=__len, args=(q, ))\n",
    "        p.start()\n",
    "        p.join()\n",
    "\n",
    "        length = q.get()          \n",
    "        return length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.first_time:                        \n",
    "            self._lazy_init()\n",
    "            self.first_time = False\n",
    "        img_path = self.df.loc[idx, 'filename_fixed']\n",
    "        image = Image.open(img_path.open()).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return {'image' : image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader,get_worker_info\n",
    "from PIL import Image\n",
    "import mltable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "img_size = 224\n",
    "\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        \"\"\"\n",
    "        Square pads an image using whitespace.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image or Tensor\n",
    "        \"\"\"\n",
    "\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "\n",
    "        return F.pad(image, padding, 0, \"constant\")\n",
    "\n",
    "tfs = transforms.Compose([\n",
    "            SquarePad(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.RandAugment(),\n",
    "            transforms.RandomVerticalFlip(0.1),\n",
    "            transforms.RandomHorizontalFlip(0.1),\n",
    "            transforms.GaussianBlur(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageDatasetMP(data_asset_id=TRAIN_ASSET_ID,transform=tfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[-0.9686, -0.9686, -0.9686,  ..., -0.9765, -0.9765, -0.9765],\n",
       "          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9686],\n",
       "          [-0.9765, -0.9686, -0.9686,  ..., -0.9686, -0.9686, -0.9686],\n",
       "          ...,\n",
       "          [-0.9922, -0.9922, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       " \n",
       "         [[-0.9686, -0.9686, -0.9686,  ..., -0.9765, -0.9765, -0.9765],\n",
       "          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9686],\n",
       "          [-0.9765, -0.9686, -0.9686,  ..., -0.9686, -0.9686, -0.9686],\n",
       "          ...,\n",
       "          [-0.9922, -0.9922, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       " \n",
       "         [[-0.9686, -0.9686, -0.9686,  ..., -0.9765, -0.9765, -0.9765],\n",
       "          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9686],\n",
       "          [-0.9765, -0.9686, -0.9686,  ..., -0.9686, -0.9686, -0.9686],\n",
       "          ...,\n",
       "          [-0.9922, -0.9922, -0.9922,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.create_model('resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
